{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyTorch as compute engine and mpi4py for communication, Heat implements a number of machine learning algorithms that are optimized for memory-distributed data volumes. This allows you to tackle datasets that are too large for single-node (or worse, single-GPU) processing. \n",
    "\n",
    "As opposed to task-parallel frameworks, Heat takes a data-parallel approach, meaning that each \"worker\" or MPI process performs the same tasks on different slices of the data, and communication between processes is performed transparently when the task requires it. \n",
    "\n",
    "In other words: you don't have to worry about optimizing data chunk sizes; you don't have to make sure your research problem is embarassingly parallel, or artificially make your dataset smaller so your RAM is sufficient. You do have to make sure that you have sufficient **overall** RAM to run your global task (e.g. number of nodes / GPUs).\n",
    "\n",
    "The following shows a few examples. We'll use \"small\" datasets here as each of us only has access to one node only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work in progress"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
