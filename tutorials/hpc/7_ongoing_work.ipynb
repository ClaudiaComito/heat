{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ongoing work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal processing - Fast Fourier Transforms (FFT)\n",
    "\n",
    "You might have noticed that Heat 1.4.0 hasn't actually been released yet. Here we want to show you a new feature that is available in our custom kernel, but not in the latest stable release v1.3.1. \n",
    "\n",
    "New in v1.4.0 is the `ht.fft` module. This is a crucial feature for many communities, in particular we are looking to support the radiointerferometry community in their efforts to scale existing processing pipelines to larger and larger datasets. \n",
    "\n",
    "Let's try 2-dimensional FFTs on a large data cube. If the transform axes do not include the split axis of the DNDarray, then the operation exploits entirely `torch.fft` operations and does not require communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "\n",
    "a = ht.random.randn(4000, 400, 400, split=0, device=\"gpu\")\n",
    "fft_2d = ht.fft.fft2(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ht.fft` module includes all the functionality that users expect of `numpy.fft`, powered by `torch.fft` and, when necessary, Heat infrastructure for communication. Researchers can perform Fourier transforms on data cubes that are too large to be transformed on a single node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast FFT-based 2D convolution will also be available in the future, to add to Heat's signal processing capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important updates that will be merged soon:\n",
    "- Batch-parallel k-means and k-medians\n",
    "- Fully distributed item selection and assignment\n",
    "- Optimized QR decomposition\n",
    "\n",
    "Coming up (hopefully) in v1.5.0:\n",
    "- Optimized Dataloader for distributed deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known issues\n",
    "\n",
    "- Slab parallelism only (data decomposition along 1 dimension) is limiting for many algorithms\n",
    "- some low-level operations are slow and need refactoring \n",
    "- full exploitation of PyTorch 2 capabilities is still pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dos and Don'ts\n",
    "\n",
    "In this section we would like to address a few best practices for programming with Heat. While we can obviously not cover all issues, these are major pointers as how to get reasonable performance.\n",
    "\n",
    "**Dos**\n",
    "\n",
    "* Split up large data amounts\n",
    "    * often you input data set along the 'observations/samples' dimension\n",
    "    * large intermediate matrices\n",
    "* Use the Heat API\n",
    "    * computational kernels are optimized\n",
    "    * Python constructs (e.g. loops) tend to be slow\n",
    "* Potentially have a copy of certain data with different splits\n",
    "\n",
    "**Dont's**\n",
    "\n",
    "* Avoid extensive data copying, e.g.\n",
    "    * operations with operands of different splits (except None)\n",
    "    * reshape() that actually change the array dimensions (adding extra dimensions with size 1 is fine)\n",
    "* Execute everything on GPU\n",
    "    * computation-intensive operations are usually a good fit\n",
    "    * operations extensively accessing memory only (e.g. sorting) are not"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
