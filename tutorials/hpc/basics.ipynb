{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat uses PyTorch and mpi4py to enable memory-distributed array operations on multi-node (including multi-GPU) systems. Let's see what this means in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17],\n",
       "         [18, 19, 20],\n",
       "         [21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26],\n",
       "         [27, 28, 29],\n",
       "         [30, 31, 32],\n",
       "         [33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38],\n",
       "         [39, 40, 41],\n",
       "         [42, 43, 44],\n",
       "         [45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50],\n",
       "         [51, 52, 53],\n",
       "         [54, 55, 56],\n",
       "         [57, 58, 59]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "array = np.arange(60).reshape(5,4,3)\n",
    "tensor = torch.arange(60).reshape(5,4,3)\n",
    "\n",
    "tensor  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat implements numpy's API as far as possible. We can create a Heat array (officially `DNDarray` or distributed n-dimensional array) using with the same functions that we use to create numpy arrays. We'll create a 3D DNDarray of integers ranging from 0 to 59 (5 matrices of size (4,3))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[[ 0,  1,  2],\n",
       "           [ 3,  4,  5],\n",
       "           [ 6,  7,  8],\n",
       "           [ 9, 10, 11]],\n",
       "\n",
       "          [[12, 13, 14],\n",
       "           [15, 16, 17],\n",
       "           [18, 19, 20],\n",
       "           [21, 22, 23]],\n",
       "\n",
       "          [[24, 25, 26],\n",
       "           [27, 28, 29],\n",
       "           [30, 31, 32],\n",
       "           [33, 34, 35]],\n",
       "\n",
       "          [[36, 37, 38],\n",
       "           [39, 40, 41],\n",
       "           [42, 43, 44],\n",
       "           [45, 46, 47]],\n",
       "\n",
       "          [[48, 49, 50],\n",
       "           [51, 52, 53],\n",
       "           [54, 55, 56],\n",
       "           [57, 58, 59]]], dtype=ht.int32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%px\n",
    "import heat as ht\n",
    "dndarray = ht.arange(60).reshape(5,4,3)\n",
    "dndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the additional metadata printed with the DNDarray. With respect to a numpy ndarray, the DNDarray has additional information on the device (in this case, the CPU) and the `split` axis. In the example above, the split axis is `None`, meaning that the DNDarray is not distributed and each MPI process has a full copy of the data.\n",
    "\n",
    "Let's experiment with a distributed DNDarray: we'll split the same DNDarrayas above, but distributed along the first axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dndarray = ht.arange(60, split=0).reshape(5,4,3)\n",
    "dndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split` axis is now 0, meaning that the DNDarray is distributed along the first axis. Each MPI process has a slice of the data along the first axis. In order to see the data on each process, we can print the \"local array\" via the `larray` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17],\n",
       "         [18, 19, 20],\n",
       "         [21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26],\n",
       "         [27, 28, 29],\n",
       "         [30, 31, 32],\n",
       "         [33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38],\n",
       "         [39, 40, 41],\n",
       "         [42, 43, 44],\n",
       "         [45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50],\n",
       "         [51, 52, 53],\n",
       "         [54, 55, 56],\n",
       "         [57, 58, 59]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%px\n",
    "dndarray.larray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `larray` is a `torch.Tensor` object. This is the underlying tensor that holds the data. The `dndarray` object is an MPI-aware wrapper around these process-local tensors, providing memory-distributed functionality and information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DNDarray can be distributed along any axis. Modify the `split` attribute in the cell above to distribute the DNDarray along a different axis, and see how the `larray`s change. You'll notice that the distributed arrays are always load-balanced, meaning that the data are distributed as evenly as possible across the MPI processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DNDarray` object has a number of methods and attributes that are useful for distributed computing. In particular, it keeps track of its global and local (on a given process) shape through distributed operations and array manipulations. The DNDarray is also associated to a `comm` object, which is an MPI communicator that allows the DNDarray to communicate with other DNDarrays. This is useful for distributed operations, such as reductions, scatter, gather, and all-to-all operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global shape of the dndarray: (5, 4, 3)\n",
      "On rank 0/1, local shape of the dndarray: (5, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(f\"Global shape of the dndarray: {dndarray.shape}\")\n",
    "print(f\"On rank {dndarray.comm.rank}/{dndarray.comm.size}, local shape of the dndarray: {dndarray.lshape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily create DNDarrays from PyTorch tensors and numpy ndarrays. We can also convert DNDarrays to PyTorch tensors and numpy ndarrays. This makes it easy to integrate Heat into existing PyTorch and numpy workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, because the underlying data objects are PyTorch tensors, we can easily create DNDarrays on GPUs or move DNDarrays to GPUs. This allows us to perform distributed array operations on multi-GPU systems.\n",
    "\n",
    "In this tutorial, you have a node of the JUWELS booster system available with 4 Nvidia A100 GPUs. You can create the DNDarray above on the GPUs by setting the `device`  attribute to \"gpu\". Note that Heat, like PyTorch, supports the ROCm ecosystem as well, so you can also perform distributed array operations on systems with AMD GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dndarray = ht.arange(60, split=0, device=\"gpu\").reshape(5,4,3)\n",
    "\n",
    "dndarray.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform a vast number of operations on  DNDarrays. Check out our [Numpy coverage tables](https://github.com/helmholtz-analytics/heat/blob/main/coverage_tables.md) to see what operations are already supported. While we are on the GPUs, let's try a matrix multiplication of two large DNDarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 ms ± 694 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "n, m = 4000, 3000\n",
    "x = ht.random.randn(n, m, split=0)\n",
    "y = ht.random.randn(m, n, split=None)\n",
    "z = %timeit -n 1  x @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heat-dev-torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
