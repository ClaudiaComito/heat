{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client(profile=\"default\")\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat uses PyTorch and mpi4py to enable memory-distributed array operations on multi-node (including multi-GPU) systems. Let's see what this means in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17],\n",
       "         [18, 19, 20],\n",
       "         [21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26],\n",
       "         [27, 28, 29],\n",
       "         [30, 31, 32],\n",
       "         [33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38],\n",
       "         [39, 40, 41],\n",
       "         [42, 43, 44],\n",
       "         [45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50],\n",
       "         [51, 52, 53],\n",
       "         [54, 55, 56],\n",
       "         [57, 58, 59]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "array = np.arange(60).reshape(5,4,3)\n",
    "tensor = torch.arange(60).reshape(5,4,3)\n",
    "\n",
    "tensor  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat implements numpy's API as far as possible. We can create a Heat array (officially `DNDarray` or distributed n-dimensional array) using with the same functions that we use to create numpy arrays. We'll create a 3D DNDarray of integers ranging from 0 to 59 (5 matrices of size (4,3))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[[ 0,  1,  2],\n",
       "           [ 3,  4,  5],\n",
       "           [ 6,  7,  8],\n",
       "           [ 9, 10, 11]],\n",
       "\n",
       "          [[12, 13, 14],\n",
       "           [15, 16, 17],\n",
       "           [18, 19, 20],\n",
       "           [21, 22, 23]],\n",
       "\n",
       "          [[24, 25, 26],\n",
       "           [27, 28, 29],\n",
       "           [30, 31, 32],\n",
       "           [33, 34, 35]],\n",
       "\n",
       "          [[36, 37, 38],\n",
       "           [39, 40, 41],\n",
       "           [42, 43, 44],\n",
       "           [45, 46, 47]],\n",
       "\n",
       "          [[48, 49, 50],\n",
       "           [51, 52, 53],\n",
       "           [54, 55, 56],\n",
       "           [57, 58, 59]]], dtype=ht.int32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%px\n",
    "import heat as ht\n",
    "dndarray = ht.arange(60).reshape(5,4,3)\n",
    "dndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the additional metadata printed with the DNDarray. With respect to a numpy ndarray, the DNDarray has additional information on the device (in this case, the CPU) and the `split` axis. In the example above, the split axis is `None`, meaning that the DNDarray is not distributed and each MPI process has a full copy of the data.\n",
    "\n",
    "Let's experiment with a distributed DNDarray: we'll split the same DNDarrayas above, but distributed along the first axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dndarray = ht.arange(60, split=0).reshape(5,4,3)\n",
    "dndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split` axis is now 0, meaning that the DNDarray is distributed along the first axis. Each MPI process has a slice of the data along the first axis. In order to see the data on each process, we can print the \"local array\" via the `larray` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17],\n",
       "         [18, 19, 20],\n",
       "         [21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26],\n",
       "         [27, 28, 29],\n",
       "         [30, 31, 32],\n",
       "         [33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38],\n",
       "         [39, 40, 41],\n",
       "         [42, 43, 44],\n",
       "         [45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50],\n",
       "         [51, 52, 53],\n",
       "         [54, 55, 56],\n",
       "         [57, 58, 59]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%px\n",
    "dndarray.larray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `larray` is a `torch.Tensor` object. This is the underlying tensor that holds the data. The `dndarray` object is an MPI-aware wrapper around these process-local tensors, providing memory-distributed functionality and information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DNDarray can be distributed along any axis. Modify the `split` attribute in the cell above to distribute the DNDarray along a different axis, and see how the `larray`s change. You'll notice that the distributed arrays are always load-balanced, meaning that the data are distributed as evenly as possible across the MPI processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DNDarray` object has a number of methods and attributes that are useful for distributed computing. In particular, it keeps track of its global and local (on a given process) shape through distributed operations and array manipulations. The DNDarray is also associated to a `comm` object, which is an MPI communicator that allows the DNDarray to communicate with other DNDarrays. This is useful for distributed operations, such as reductions, scatter, gather, and all-to-all operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global shape of the dndarray: (5, 4, 3)\n",
      "On rank 0/1, local shape of the dndarray: (5, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(f\"Global shape of the dndarray: {dndarray.shape}\")\n",
    "print(f\"On rank {dndarray.comm.rank}/{dndarray.comm.size}, local shape of the dndarray: {dndarray.lshape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily create DNDarrays from PyTorch tensors and numpy ndarrays. We can also convert DNDarrays to PyTorch tensors and numpy ndarrays. This makes it easy to integrate Heat into existing PyTorch and numpy workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, because the underlying data objects are PyTorch tensors, we can easily create DNDarrays on GPUs or move DNDarrays to GPUs. This allows us to perform distributed array operations on multi-GPU systems.\n",
    "\n",
    "In this tutorial, you have a node of the JUWELS booster system available with 4 Nvidia A100 GPUs. You can create the DNDarray above on the GPUs by setting the `device`  attribute to \"gpu\". Note that Heat, like PyTorch, supports the ROCm ecosystem as well, so in principle you can also perform distributed array operations on systems with AMD GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dndarray = ht.arange(60, split=0, device=\"gpu\").reshape(5,4,3)\n",
    "\n",
    "dndarray.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform a vast number of operations on  DNDarrays. Check out our [Numpy coverage tables](https://github.com/helmholtz-analytics/heat/blob/main/coverage_tables.md) to see what operations are already supported. While we are on the GPUs, let's try a matrix multiplication of two large DNDarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 ms ± 694 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "n, m = 40000, 40000\n",
    "x = ht.random.randn(n, m, split=None, device=\"gpu\")\n",
    "y = ht.random.randn(m, n, split=None, device=\"gpu\")\n",
    "z = %timeit -n 1  x @ y\n",
    "del x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with the split parameter (distribution axis) for both matrices and time the result. Note that:\n",
    "- If you set `split=None` for both matrices, each process (in this case, each GPU) will attempt to multiply the entire matrices. You will notice that the GPU memory is insufficient.\n",
    "- If `split` is not None for both matrices, each process will only hold a slice of the data, and will need to communicate data with other processes in order to perform the multiplication. This introduces communication overhead, but allows you to perform the multiplication on larger matrices than would fit in the memory of a single GPU.\n",
    "- If `split` is None for one matrix and not None for the other, the multiplication does not require communication, and the result will be distributed. If your data size allows it, you should always favor this option.\n",
    "\n",
    "Time the multiplication for different split parameters and see how the performance changes.\n",
    "\n",
    "You can of course perform the same operations on CPUs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heat-dev-torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
